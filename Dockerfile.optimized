# syntax=docker/dockerfile:1.4
# Use BuildKit for advanced caching features

FROM python:3.12-slim-bookworm AS base

# C4ai version
ARG C4AI_VER=0.7.0-r1
ENV C4AI_VERSION=$C4AI_VER
LABEL c4ai.version=$C4AI_VER

# Set build arguments
ARG APP_HOME=/app
ARG GITHUB_REPO=https://github.com/unclecode/crawl4ai.git
ARG GITHUB_BRANCH=main
ARG USE_LOCAL=true

# ✅ OPTIMIZATION: Enable pip caching (removed PIP_NO_CACHE_DIR=1)
ENV PYTHONFAULTHANDLER=1 \
    PYTHONHASHSEED=random \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PIP_DEFAULT_TIMEOUT=100 \
    DEBIAN_FRONTEND=noninteractive \
    REDIS_HOST=localhost \
    REDIS_PORT=6379

ARG PYTHON_VERSION=3.12
ARG INSTALL_TYPE=default
ARG ENABLE_GPU=false
ARG TARGETARCH

LABEL maintainer="unclecode"
LABEL description="🔥🕷️ Crawl4AI: Open-source LLM Friendly Web Crawler & scraper"
LABEL version="1.0"

# =====================================
# SYSTEM DEPENDENCIES STAGE
# =====================================

FROM base AS system-deps

# ✅ OPTIMIZATION: Combine all apt operations and use cache mount
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    rm -f /etc/apt/apt.conf.d/docker-clean && \
    apt-get update && apt-get install -y --no-install-recommends \
    # Build tools
    build-essential \
    curl \
    wget \
    gnupg \
    git \
    cmake \
    pkg-config \
    python3-dev \
    libjpeg-dev \
    redis-server \
    supervisor \
    # Browser dependencies
    libglib2.0-0 \
    libnss3 \
    libnspr4 \
    libatk1.0-0 \
    libatk-bridge2.0-0 \
    libcups2 \
    libdrm2 \
    libdbus-1-3 \
    libxcb1 \
    libxkbcommon0 \
    libx11-6 \
    libxcomposite1 \
    libxdamage1 \
    libxext6 \
    libxfixes3 \
    libxrandr2 \
    libgbm1 \
    libpango-1.0-0 \
    libcairo2 \
    libasound2 \
    libatspi2.0-0 \
    && apt-get dist-upgrade -y

# ✅ OPTIMIZATION: Platform-specific optimizations in single layer
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    if [ "$ENABLE_GPU" = "true" ] && [ "$TARGETARCH" = "amd64" ] ; then \
        apt-get update && apt-get install -y --no-install-recommends nvidia-cuda-toolkit ; \
    fi && \
    if [ "$TARGETARCH" = "arm64" ]; then \
        echo "🦾 Installing ARM-specific optimizations" && \
        apt-get update && apt-get install -y --no-install-recommends libopenblas-dev ; \
    elif [ "$TARGETARCH" = "amd64" ]; then \
        echo "🖥️ Installing AMD64-specific optimizations" && \
        apt-get update && apt-get install -y --no-install-recommends libomp-dev ; \
    fi

# =====================================
# BASE PYTHON DEPENDENCIES STAGE  
# =====================================

FROM system-deps AS python-base

# Create non-root user early
RUN groupadd -r appuser && useradd --no-log-init -r -g appuser appuser && \
    mkdir -p /home/appuser && chown -R appuser:appuser /home/appuser

WORKDIR ${APP_HOME}

# ✅ OPTIMIZATION: Install base requirements first (cached until requirements.txt changes)
COPY deploy/docker/requirements.txt /tmp/
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --upgrade pip && \
    pip install -r /tmp/requirements.txt

# =====================================
# AI/ML DEPENDENCIES STAGE
# =====================================

FROM python-base AS ml-deps

# ✅ OPTIMIZATION: Install heavy AI/ML packages in separate cached layer
RUN --mount=type=cache,target=/root/.cache/pip \
    if [ "$INSTALL_TYPE" = "all" ] ; then \
        echo "🤖 Installing AI/ML packages (this may take a while on first build)..." && \
        pip install \
            torch \
            torchvision \
            torchaudio \
            scikit-learn \
            nltk \
            transformers \
            tokenizers ; \
    fi

# ✅ OPTIMIZATION: Download NLTK data in separate layer (cached)
RUN if [ "$INSTALL_TYPE" = "all" ] ; then \
        python -c "import nltk; nltk.download('punkt', quiet=True); nltk.download('stopwords', quiet=True)" || \
        echo "⚠️ NLTK download failed, will retry later" ; \
    fi

# =====================================
# PLAYWRIGHT BROWSER STAGE
# =====================================

FROM ml-deps AS browser-deps

# ✅ OPTIMIZATION: Install Playwright in separate cached layer
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install playwright

# ✅ OPTIMIZATION: Cache browser installation
RUN --mount=type=cache,target=/root/.cache/ms-playwright \
    playwright install --with-deps chromium

# =====================================
# APPLICATION STAGE
# =====================================

FROM browser-deps AS app-builder

# ✅ OPTIMIZATION: Copy project files late (only rebuilds this layer on code changes)
COPY . /tmp/project/

# Install the application based on type
RUN --mount=type=cache,target=/root/.cache/pip \
    if [ "$INSTALL_TYPE" = "all" ] ; then \
        pip install "/tmp/project/[all]" ; \
    elif [ "$INSTALL_TYPE" = "torch" ] ; then \
        pip install "/tmp/project/[torch]" ; \
    elif [ "$INSTALL_TYPE" = "transformer" ] ; then \
        pip install "/tmp/project/[transformer]" ; \
    else \
        pip install "/tmp/project" ; \
    fi

# ✅ OPTIMIZATION: Run model loader only if needed
RUN if [ "$INSTALL_TYPE" = "all" ] || [ "$INSTALL_TYPE" = "transformer" ] ; then \
        python -m crawl4ai.model_loader || echo "⚠️ Model loader failed, continuing..." ; \
    fi

# Verify installation
RUN python -c "import crawl4ai; print('✅ crawl4ai is ready to rock!')" && \
    python -c "from playwright.sync_api import sync_playwright; print('✅ Playwright is feeling dramatic!')"

# =====================================
# FINAL RUNTIME STAGE
# =====================================

FROM app-builder AS runtime

# Run crawl4ai setup
RUN crawl4ai-setup

# ✅ OPTIMIZATION: Copy browser cache to appuser efficiently
RUN mkdir -p /home/appuser/.cache/ms-playwright && \
    if [ -d "/root/.cache/ms-playwright" ]; then \
        cp -r /root/.cache/ms-playwright/* /home/appuser/.cache/ms-playwright/ 2>/dev/null || true ; \
    fi && \
    chown -R appuser:appuser /home/appuser/.cache

# Run health check
RUN crawl4ai-doctor || echo "⚠️ Doctor check had warnings, continuing..."

# Copy application files
COPY deploy/docker/supervisord.conf ${APP_HOME}/
COPY deploy/docker/*.py ${APP_HOME}/
COPY deploy/docker/static ${APP_HOME}/static

# Set up Redis directories and permissions
RUN mkdir -p /var/lib/redis /var/log/redis && \
    chown -R appuser:appuser ${APP_HOME} /var/lib/redis /var/log/redis

# ✅ OPTIMIZATION: Improved healthcheck with better error messages
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD bash -c '\
    MEM=$(free -m | awk "/^Mem:/{print \$2}"); \
    if [ $MEM -lt 2048 ]; then \
        echo "⚠️ Warning: Less than 2GB RAM available! Your container might need a memory boost! 🚀"; \
        exit 1; \
    fi && \
    redis-cli ping > /dev/null 2>&1 && \
    curl -f http://localhost:11235/health > /dev/null 2>&1 || exit 1'

EXPOSE 6379 11235

# Switch to non-root user
USER appuser

# Set production environment
ENV PYTHON_ENV=production 

# Start the application
CMD ["supervisord", "-c", "supervisord.conf"]